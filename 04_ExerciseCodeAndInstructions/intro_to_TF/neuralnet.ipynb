{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c287f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:22:39.377895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748110959.400201   23749 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748110959.407035   23749 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748110959.424040   23749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748110959.424058   23749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748110959.424060   23749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748110959.424062   23749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 13:22:39.432114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc265b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data for training\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "350fbab3",
   "metadata": {},
   "source": [
    "## Dense Feed Forward Neural Networks (DFNNs)  also known as multi-layer perceptrons(MLPs)\n",
    "* well suited for inferencing from tabular data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db8118db",
   "metadata": {},
   "source": [
    "### Construct DFNN model using Sequential API  \n",
    "* simple interface\n",
    "* limited functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c343812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748110963.838592   23749 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 224 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0, compute capability: 6.0\n",
      "I0000 00:00:1748110963.839142   23749 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 15252 MB memory:  -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0\n",
      "I0000 00:00:1748110963.860044   23749 cuda_executor.cc:479] failed to allocate 224.38MiB (235274240 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "# Sequential API (Very convenient, not very flexible)\n",
    "modelDFNN1 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(784,)),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "modelDFNN1 = keras.Sequential()\n",
    "modelDFNN1.add(keras.Input(shape=(784,)))\n",
    "modelDFNN1.add(layers.Dense(512, activation=\"relu\"))\n",
    "modelDFNN1.add(layers.Dense(256, activation=\"relu\", name=\"my_layer\"))\n",
    "modelDFNN1.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6c1f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ my_layer (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,818</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m535,818\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,818</span> (2.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m535,818\u001b[0m (2.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# ToDo #1: List what objects the modelDFNN1 is comprised of (built from)\n",
    "print(modelDFNN1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c00633",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDFNN1.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb47d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748110965.819896   24018 service.cc:152] XLA service 0x2aaea0001440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748110965.819929   24018 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1748110965.819933   24018 service.cc:160]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2025-05-24 13:22:45.847918: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748110965.987104   24018 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1748110966.023903   24018 cuda_executor.cc:479] failed to allocate 22.44MiB (23527424 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "I0000 00:00:1748110966.023972   24018 cuda_executor.cc:479] failed to allocate 20.19MiB (21174784 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "I0000 00:00:1748110966.024019   24018 cuda_executor.cc:479] failed to allocate 18.17MiB (19057408 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "I0000 00:00:1748110966.024063   24018 cuda_executor.cc:479] failed to allocate 16.36MiB (17151744 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2025-05-24 13:22:46.024082: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.10MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-05-24 13:22:46.028940: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16877568 bytes. [tf-allocator-allocation-error='']\n",
      "2025-05-24 13:22:46.029004: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16877568 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n\n  File \"/tmp/ipykernel_23749/3479254115.py\", line 1, in <module>\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 16877568 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_1352]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhaustedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodelDFNN1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m modelDFNN1.evaluate(x_test, y_test, batch_size=\u001b[32m32\u001b[39m, verbose=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mResourceExhaustedError\u001b[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n\n  File \"/tmp/ipykernel_23749/3479254115.py\", line 1, in <module>\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/project/nanocourse/SWE_OOP/shared/CondaEnvs/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 16877568 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_1352]"
     ]
    }
   ],
   "source": [
    "modelDFNN1.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
    "modelDFNN1.evaluate(x_test, y_test, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo #2: What train and test accuracy do you obtain when running for 5 epochs ?  \n",
    "\n",
    "modelDFNN1.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
    "modelDFNN1.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo #3: Now write code (add code cells to this notebook) to train the model for 20 additional epochs. What train and test accuracy do you obtain? \n",
    "modelDFNN1.fit(x_train, y_train, batch_size=32, epochs=25, verbose=2)\n",
    "modelDFNN1.evaluate(x_test, y_test, batch_size=32, verbose=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b9d4f5",
   "metadata": {},
   "source": [
    "### Construct DFNN model using Functional API  \n",
    "* slightly more complex interface\n",
    "* additional functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7835d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo #4: Now practice using the OO functional API of keras by constructing your own functional model, modelDFNN2.\n",
    "# To simplify the task, create this model so that it uses the same architecture as the sequential model.\n",
    "# For the final Dense layer use activation=\"softmax\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ad72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API (A bit more flexible)\n",
    "inputs = keras.Input(shape=(784))\n",
    "x = layers.Dense(512, activation=\"relu\", name=\"first_layer\")(inputs)\n",
    "x = layers.Dense(256, activation=\"relu\", name=\"second_layer\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "modelDFNN2 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to gain experience invoking methods on objects you have created,\n",
    "# write code to compile(), fit() your new model on the training data. \n",
    "# Then write code to evaluate your new model on the test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab84e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDFNN2.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDFNN2.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
    "modelDFNN2.evaluate(x_test, y_test, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35993c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo #5. Write down what train and test accuracy do you observe? How does it compare to the sequential model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaafd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fun: explore the architecture space by adding or removing layers from your model, retrain from scratch. \n",
    "# Learn how that impacts model performance. State of the art accuracy is around 99.97% . \n",
    "# You do not need to achieve that (nor is that expected here) but to give you an idea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
